{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4142,"status":"ok","timestamp":1677075604187,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"BfSzCObCeI2U","outputId":"ff19578f-153c-480c-ee6c-72b86f9f8fe4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1677075604187,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"KJ1g8KVVnQde"},"outputs":[],"source":["root = '/content/drive/MyDrive/140111/'"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":2639,"status":"ok","timestamp":1677075606821,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"mWLfSnd9dxeC"},"outputs":[],"source":["from tensorflow import keras\n","from tensorflow.keras import layers\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","import numpy as np\n","import imageio\n","from tensorflow.keras.initializers import he_normal\n","import datetime\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1677075606825,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"hooQGNrzd1kG"},"outputs":[],"source":["batch_size = 64\n","num_channels = 1\n","image_size = 128\n","latent_dim = 128"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":1044,"status":"ok","timestamp":1677075607860,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"hs7lsaTjnVmm"},"outputs":[],"source":["data = np.load('/content/drive/MyDrive/140111/data.npy')\n","types = open('/content/drive/MyDrive/140111/types.txt').readlines()\n","sub_types = open('/content/drive/MyDrive/140111/subtypes.txt').readlines()"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":1458,"status":"ok","timestamp":1677075609312,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"OAXIEsYq65Zz"},"outputs":[],"source":["data[data > 16] = 16 \n","data /= 16.0"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677075609314,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"CYjUEM2j7Iec"},"outputs":[],"source":["df = pd.DataFrame(columns=['id','type','subtype','type_code', 'subtype_code'])\n","df.id = np.arange(data.shape[0])\n","df.type = types\n","df.type = df.type.str.strip()\n","df.subtype = sub_types\n","df.subtype = df.subtype.str.strip()\n","df = df.drop(df[df.type=='NA'].index)\n","df = df.drop(df[df.subtype=='NA'].index)\n","df.type_code = LabelEncoder().fit_transform(df.type)\n","df.subtype_code = LabelEncoder().fit_transform(df.subtype)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677075609315,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"wbArYq2I7RkY"},"outputs":[],"source":["primary_df = df[df.subtype=='Primary Tumor']\n","primary_df = primary_df.sample(n=726)\n","df = df.drop(df[df.subtype=='Primary Tumor'].index)\n","df = pd.concat([df,primary_df],axis=0)"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677075609316,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"8wqIr3eT7b8o"},"outputs":[],"source":["x_train = data[df.id.values]\n","x_train = x_train * 2.0 - 1.0\n","y_train = df.type_code.values\n","y_train = to_categorical(y_train)\n","c_cat_dim = len(df.subtype.unique())\n","num_classes = y_train.shape[1]"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2126,"status":"ok","timestamp":1677075611434,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"EjmugXlHd4Nq","outputId":"57344795-5170-4bd4-af6a-5b5d780dd015"},"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of training images: (2074, 128, 128, 1)\n","Shape of training labels: (2074, 33)\n"]}],"source":["# Create tf.data.Dataset.\n","dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n","dataset = dataset.shuffle(buffer_size=1024).batch(batch_size)\n","\n","print(f\"Shape of training images: {x_train.shape}\")\n","print(f\"Shape of training labels: {y_train.shape}\")"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1677075611435,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"Q4YaD3Z7eV1R","outputId":"0b95e7b0-adb1-4cce-af3b-fda267cca14e"},"outputs":[{"output_type":"stream","name":"stdout","text":["168\n"]}],"source":["generator_in_channels = latent_dim + num_classes + c_cat_dim\n","print(generator_in_channels)"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1677075611436,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"urYt5RZnrwCr"},"outputs":[],"source":["def get_discriminator():\n","  label_layer = layers.Input(shape=(num_classes,))\n","  li = layers.Dense(image_size*image_size ,use_bias=False)(label_layer)\n","  li = layers.Reshape((image_size,image_size,1))(li)\n","  input_layer = layers.Input(shape=(image_size,image_size,num_channels))\n","  x = layers.Concatenate()([li,input_layer])\n","  x = layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\")(x)\n","  x = layers.LeakyReLU(alpha=0.2)(x)\n","  x = layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\")(x)\n","  x = layers.LeakyReLU(alpha=0.2)(x)\n","  x = layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\")(x)\n","  x = layers.LeakyReLU(alpha=0.2)(x)\n","  x = layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\")(x)\n","  x = layers.LeakyReLU(alpha=0.2)(x)\n","  x = layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\")(x)\n","  x = layers.LeakyReLU(alpha=0.2)(x)\n","  x = layers.GlobalMaxPooling2D()(x)\n","  disc_out = layers.Dense(1, name='disc_out')(x)\n","\n","  d_model = keras.models.Model([input_layer,label_layer], disc_out, name=\"discriminator\")\n","\n","  # q_net_out = layers.Dense(128, activation='relu', kernel_initializer='he_normal' , bias_initializer='he_normal'  )(x)\n","  # q_net_out = layers.Dense(c_cat_dim , activation='softmax')(q_net_out)\n","  # q_model = keras.models.Model(img_input, q_net_out, name='q_network')\n","  q_net_out = layers.Dense(512 , kernel_initializer='he_normal' , bias_initializer='he_normal'  )(x)\n","  q_net_out = layers.LeakyReLU(0.2)(q_net_out)\n","  q_net_out = layers.Dropout(0.2)(q_net_out)\n","  q_net_out = layers.Dense(256 , kernel_initializer='he_normal' , bias_initializer='he_normal'  )(q_net_out)\n","  q_net_out = layers.LeakyReLU(0.2)(q_net_out)\n","  q_net_out = layers.Dropout(0.2)(q_net_out)\n","  q_net_out = layers.Dense(c_cat_dim , kernel_initializer='he_normal' , bias_initializer='he_normal'  )(q_net_out)\n","  q_net_out = layers.Activation('softmax')(q_net_out)\n","  q_model = keras.models.Model([input_layer,label_layer], q_net_out, name='q_network')\n","\n","  return d_model, q_model"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1677075611437,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"6UV3WZvi3V4K"},"outputs":[],"source":["def get_generator_model():\n","  noise = layers.Input(shape=(latent_dim,) , name='noise')\n","  labels = layers.Input(shape=(c_cat_dim,), name='c_cat')\n","  class_label = layers.Input(shape=(num_classes,), name='class_label')\n","  inputs =layers.concatenate([noise,labels,class_label], axis=1)\n","  x = layers.Dense(8 * 8 * (latent_dim+c_cat_dim+num_classes), name='gen_l1')(inputs)\n","  x = layers.LeakyReLU(alpha=0.2)(x)\n","  x = layers.Reshape((8, 8, latent_dim+c_cat_dim+num_classes))(x)\n","  x = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\"same\",use_bias=False)(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.LeakyReLU(alpha=0.2)(x)\n","  x = layers.Conv2DTranspose(128, (3, 3), strides=(2, 2), padding=\"same\",use_bias=False)(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.LeakyReLU(alpha=0.2)(x)\n","  x = layers.Conv2DTranspose(128, (5, 5), strides=(2, 2), padding=\"same\",use_bias=False)(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.LeakyReLU(alpha=0.2)(x)\n","  x = layers.Conv2DTranspose(128, (7, 7), strides=(2, 2), padding=\"same\",use_bias=False)(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.LeakyReLU(alpha=0.2)(x)\n","  x = layers.Conv2D(1, (7, 7), padding=\"same\",use_bias=False)(x)\n","  x = layers.BatchNormalization()(x)\n","  x = layers.Activation('tanh',name='gen_out')(x)\n","  g_model = keras.models.Model([noise,labels,class_label], x, name=\"generator\")\n","  return g_model"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":556,"status":"ok","timestamp":1677075611982,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"mPAO4BLJfGpr","outputId":"5e2a8eb2-1aa5-4c30-fa90-397a100cb7a5"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n","WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["# discriminator, q_network = get_discriminator()\n","# discriminator.summary()\n","discriminator = tf.keras.models.load_model(root + 'new model Discriminator.h5')\n","q_network = tf.keras.models.load_model(root + 'new model q network.h5')"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":584,"status":"ok","timestamp":1677075612557,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"-KkOrVEDfJ7D","outputId":"4c956505-b4ec-4319-9150-1389ed5bf4ad"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"]}],"source":["# g_model = get_generator_model()\n","# g_model.summary()\n","g_model = tf.keras.models.load_model(root + 'new model Generator.h5')"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1677075612558,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"Snt6-2Uaedch"},"outputs":[],"source":["class ConditionalInFOGAN(keras.Model):\n","    def __init__(self, discriminator, generator,q_network, latent_dim):\n","        super(ConditionalInFOGAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.q_network = q_network\n","        self.latent_dim = latent_dim\n","        self.gen_loss_tracker = keras.metrics.Mean(name=\"generator_loss\")\n","        self.disc_loss_tracker = keras.metrics.Mean(name=\"discriminator_loss\")\n","        self.q_loss_tracker = keras.metrics.Mean(name=\"q_loss\")\n","        self.batch_size = batch_size\n","        self.d_steps = 5\n","        self.gp_weight = 100\n","\n","    @property\n","    def metrics(self):\n","        return [self.gen_loss_tracker, self.disc_loss_tracker]\n","\n","    def compile(self, d_optimizer, g_optimizer,q_optimizer, d_loss_fn, g_loss_fn,q_loss_fn):\n","        super(ConditionalInFOGAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.q_optimizer = q_optimizer\n","        self.d_loss_fn = d_loss_fn\n","        self.g_loss_fn = g_loss_fn\n","        self.q_loss_fn = q_loss_fn\n","\n","    def gradient_penalty(self, batch_size, real_images, fake_images , class_label):\n","        \"\"\" Calculates the gradient penalty.\n","\n","        This loss is calculated on an interpolated image\n","        and added to the discriminator loss.\n","        \"\"\"\n","        # Get the interpolated image\n","        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n","        diff = fake_images - real_images\n","        interpolated = real_images + alpha * diff\n","\n","        with tf.GradientTape() as gp_tape:\n","            gp_tape.watch(interpolated)\n","            # 1. Get the discriminator output for this interpolated image.\n","            pred = self.discriminator([interpolated,class_label], training=True)\n","\n","        # 2. Calculate the gradients w.r.t to this interpolated image.\n","        grads = gp_tape.gradient(pred, [interpolated])[0]\n","        # 3. Calculate the norm of the gradients.\n","        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n","        gp = tf.reduce_mean((norm - 1.0) ** 2)\n","        return gp\n","\n","    def train_step(self, data):\n","        # Unpack the data.\n","        real_images, one_hot_class_labels = data\n","\n","\n","        for i in range(self.d_steps):\n","          # Sample random points in the latent space and concatenate the labels.\n","          # This is for the generator.\n","          batch_size = tf.shape(real_images)[0]\n","          random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","          indx = tf.random.uniform(shape=(batch_size,), minval=0, maxval=c_cat_dim, dtype=tf.int32)\n","          cat_labels = tf.one_hot(indx , c_cat_dim)\n","\n","          # Decode the noise (guided by labels) to fake images.\n","          generated_images = self.generator([random_latent_vectors, cat_labels,one_hot_class_labels ])\n","\n","          # Train the discriminator.\n","          with tf.GradientTape() as tape:\n","            self.discriminator.trainable = True\n","            \n","            fake_logits = self.discriminator([generated_images,one_hot_class_labels], training=True)\n","            real_logits = self.discriminator([real_images,one_hot_class_labels], training=True)\n","\n","            d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n","            gp = self.gradient_penalty(batch_size, real_images, generated_images,one_hot_class_labels)\n","            d_loss = d_cost + gp * self.gp_weight\n","\n","          # Get the gradients w.r.t the discriminator loss\n","          d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n","          # Update the weights of the discriminator using the discriminator optimizer\n","          self.d_optimizer.apply_gradients(\n","              zip(d_gradient, self.discriminator.trainable_variables)\n","          )\n","\n","        # Sample random points in the latent space.\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","\n","        indx = tf.random.uniform(shape=(batch_size,), minval=0, maxval=c_cat_dim, dtype=tf.int32)\n","        cat_labels = tf.one_hot(indx , c_cat_dim)\n","        # Train the generator (note that we should *not* update the weights\n","        # of the discriminator)!\n","        with tf.GradientTape() as g_tape, tf.GradientTape() as qn_tape:\n","          self.discriminator.trainable = False\n","          g_tape.watch(self.generator.trainable_variables)\n","          qn_tape.watch(self.q_network.trainable_variables)\n","\n","          fake_images = self.generator([random_latent_vectors,cat_labels,one_hot_class_labels])\n","          gen_img_logits = self.discriminator([fake_images,one_hot_class_labels], training=True)\n","          \n","\n","          cat_output = self.q_network([generated_images,one_hot_class_labels], training=True)\n","          cat_loss = self.q_loss_fn(cat_labels , cat_output)\n","\n","          g_loss = self.g_loss_fn(gen_img_logits) + cat_loss\n","\n","        # Get the gradients w.r.t the generator loss\n","        gen_gradient = g_tape.gradient(g_loss, self.generator.trainable_variables)\n","        # Update the weights of the generator using the generator optimizer\n","        self.g_optimizer.apply_gradients(\n","            zip(gen_gradient, self.generator.trainable_variables)\n","        )\n","\n","        qn_gradinet = qn_tape.gradient(cat_loss , self.q_network.trainable_variables)\n","        self.q_optimizer.apply_gradients(\n","            zip(qn_gradinet , self.q_network.trainable_variables))\n","        \n","        # Monitor loss.\n","        self.gen_loss_tracker.update_state(g_loss)\n","        self.disc_loss_tracker.update_state(d_loss)\n","        self.q_loss_tracker.update_state(cat_loss)\n","        return {\n","            \"g_loss\": self.gen_loss_tracker.result(),\n","            \"d_loss\": self.disc_loss_tracker.result(),\n","            \"q_loss\": self.q_loss_tracker.result()\n","        }"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1677075612559,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"93rW3kQxxhe6"},"outputs":[],"source":["def discriminator_loss(real_img, fake_img):\n","    real_loss = tf.reduce_mean(real_img)\n","    fake_loss = tf.reduce_mean(fake_img)\n","    return fake_loss - real_loss\n","\n","\n","# Define the loss functions for the generator.\n","def generator_loss(fake_img):\n","    return -tf.reduce_mean(fake_img)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677075612559,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"RRNnvrCngID7"},"outputs":[],"source":["class GANMonitor(tf.keras.callbacks.Callback):\n","    def __init__(self, latent_dim=128):\n","        self.latent_dim = latent_dim\n","\n","    def on_epoch_end(self, epoch, logs=None):\n","        # Sample noise for the interpolation.\n","        _noise = tf.random.normal(shape=(4, latent_dim))\n","        _label = keras.utils.to_categorical([0,1,2,3], num_classes)\n","        _label = tf.cast(_label, tf.float32)\n","\n","        cat_label = keras.utils.to_categorical([0,1,2,3], c_cat_dim)\n","        cat_label = tf.cast(cat_label, tf.float32)\n","\n","        # Combine the noise and the labels and run inference with the generator.\n","        fake_images = self.model.generator.predict([_noise,cat_label,_label])\n","        fake_images = fake_images * 0.5 + 0.5\n","        fake_images *= 255.0\n","        converted_images = fake_images.astype(np.uint8)\n","        # converted_images = tf.image.resize(converted_images, (256, 256)).numpy().astype(np.uint8)\n","\n","        \n","        for i in range(4):\n","          plt.subplot(2,2,i+1)\n","          plt.imshow(converted_images[i][:,:,0],cmap='gray')\n","        plt.show()\n","\n","        if epoch % 20 == 0 :\n","          self.model.generator.save(root + 'new model Generator.h5')\n","          self.model.discriminator.save(root + 'new model Discriminator.h5')\n","          self.model.q_network.save(root + 'new model q network.h5')\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677075612560,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"Ogb1weVSilfK"},"outputs":[],"source":["callback = GANMonitor(latent_dim=latent_dim)"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1677075612561,"user":{"displayName":"hamid ravee","userId":"00918790653708190806"},"user_tz":-210},"id":"gkZWVou_ehBw"},"outputs":[],"source":["d_optimizer=keras.optimizers.Adam(learning_rate=0.0003,beta_1=0.5, beta_2=0.9)\n","g_optimizer=keras.optimizers.Adam(learning_rate=0.0003,beta_1=0.5, beta_2=0.9)\n","q_optimizer=keras.optimizers.Adam(learning_rate=0.0003, beta_1=0.5, beta_2=0.9)\n","cond_infogan = ConditionalInFOGAN(\n","    discriminator=discriminator, generator=g_model , q_network=q_network, latent_dim=latent_dim\n",")\n","cond_infogan.compile(\n","    d_optimizer=d_optimizer,\n","    g_optimizer=g_optimizer,\n","    q_optimizer=q_optimizer,\n","    g_loss_fn=generator_loss,\n","    d_loss_fn=discriminator_loss,\n","    q_loss_fn=keras.losses.CategoricalCrossentropy()\n",")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Ol9DgDTiA7b2dkVBAPFWtqZZ_dyTdbIU"},"id":"dse3Y3aTv-oB","outputId":"9817831a-45fb-4903-a417-23880e77fbbc"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["cond_infogan.fit(dataset, epochs=500 , callbacks=[callback])\n","# cond_gan.fit(dataset, epochs=20)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}